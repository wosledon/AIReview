# Token æ¶ˆè€—ç»Ÿè®¡åŠŸèƒ½è®¾è®¡æ–‡æ¡£

## ğŸ“‹ åŠŸèƒ½æ¦‚è¿°

Token æ¶ˆè€—ç»Ÿè®¡åŠŸèƒ½ç”¨äºè¿½è¸ªå’Œåˆ†æ AI æ¨¡å‹çš„ token ä½¿ç”¨é‡å’Œæˆæœ¬,å¸®åŠ©ç”¨æˆ·å’Œç®¡ç†å‘˜:
- ç›‘æ§ AI è°ƒç”¨çš„æˆæœ¬
- åˆ†æä½¿ç”¨æ¨¡å¼å’Œè¶‹åŠ¿
- ä¼˜åŒ– token ä½¿ç”¨æ•ˆç‡
- è¿›è¡Œæˆæœ¬é¢„æµ‹å’Œé¢„ç®—æ§åˆ¶

## ğŸ—ï¸ æ¶æ„è®¾è®¡

### æ ¸å¿ƒç»„ä»¶

1. **TokenUsageRecord**: è®°å½•æ¯æ¬¡ LLM è°ƒç”¨çš„è¯¦ç»†ä¿¡æ¯
2. **LLMPricingConfig**: ç®¡ç†ä¸åŒ LLM æä¾›å•†çš„å®šä»·ä¿¡æ¯
3. **TokenUsageService**: æä¾› token ä½¿ç”¨è®°å½•å’Œç»Ÿè®¡åŠŸèƒ½
4. **TokenUsageRepository**: æ•°æ®è®¿é—®å±‚
5. **TokenUsageController**: API ç«¯ç‚¹

### æ•°æ®æ¨¡å‹

```
TokenUsageRecord
â”œâ”€â”€ Id (long)
â”œâ”€â”€ UserId (string) - ç”¨æˆ·ID
â”œâ”€â”€ ProjectId (int?) - é¡¹ç›®ID(å¯é€‰)
â”œâ”€â”€ ReviewRequestId (int?) - è¯„å®¡è¯·æ±‚ID(å¯é€‰)
â”œâ”€â”€ LLMConfigurationId (int) - LLMé…ç½®ID
â”œâ”€â”€ Provider (string) - æä¾›å•†(OpenAI, DeepSeekç­‰)
â”œâ”€â”€ Model (string) - æ¨¡å‹åç§°
â”œâ”€â”€ OperationType (string) - æ“ä½œç±»å‹(Review, RiskAnalysisç­‰)
â”œâ”€â”€ PromptTokens (int) - è¾“å…¥tokenæ•°
â”œâ”€â”€ CompletionTokens (int) - è¾“å‡ºtokenæ•°
â”œâ”€â”€ TotalTokens (int) - æ€»tokenæ•°
â”œâ”€â”€ PromptCost (decimal) - è¾“å…¥æˆæœ¬
â”œâ”€â”€ CompletionCost (decimal) - è¾“å‡ºæˆæœ¬
â”œâ”€â”€ TotalCost (decimal) - æ€»æˆæœ¬
â”œâ”€â”€ IsSuccessful (bool) - æ˜¯å¦æˆåŠŸ
â”œâ”€â”€ ErrorMessage (string?) - é”™è¯¯ä¿¡æ¯
â”œâ”€â”€ ResponseTimeMs (int?) - å“åº”æ—¶é—´
â”œâ”€â”€ IsCached (bool) - æ˜¯å¦ä½¿ç”¨ç¼“å­˜
â””â”€â”€ CreatedAt (DateTime) - åˆ›å»ºæ—¶é—´
```

## ğŸ’° å®šä»·é…ç½®

### å½“å‰æ”¯æŒçš„æ¨¡å‹å®šä»·(æ¯ç™¾ä¸‡ tokens)

| æä¾›å•† | æ¨¡å‹ | è¾“å…¥ä»·æ ¼ | è¾“å‡ºä»·æ ¼ |
|--------|------|----------|----------|
| OpenAI | gpt-4o | $5.00 | $15.00 |
| OpenAI | gpt-4o-mini | $0.15 | $0.60 |
| OpenAI | gpt-4-turbo | $10.00 | $30.00 |
| OpenAI | gpt-3.5-turbo | $0.50 | $1.50 |
| DeepSeek | deepseek-coder | $0.14 | $0.28 |
| DeepSeek | deepseek-chat | $0.14 | $0.28 |
| Azure | gpt-4o | $5.00 | $15.00 |
| Azure | gpt-4o-mini | $0.15 | $0.60 |

### å®šä»·æ›´æ–°

å®šä»·ä¿¡æ¯å­˜å‚¨åœ¨ `LLMPricingService` é™æ€ç±»ä¸­,å¯ä»¥é€šè¿‡ä»£ç æ›´æ–°:

```csharp
LLMPricingService.AddOrUpdatePricing(new LLMPricingConfig
{
    Provider = "OpenAI",
    Model = "gpt-4o",
    PromptPricePerMillionTokens = 5.00m,
    CompletionPricePerMillionTokens = 15.00m
});
```

## ğŸ”Œ é›†æˆæŒ‡å—

### 1. åœ¨ LLM æœåŠ¡ä¸­é›†æˆ Token è®°å½•

éœ€è¦åœ¨æ¯æ¬¡ LLM è°ƒç”¨åè®°å½• token ä½¿ç”¨é‡ã€‚ä»¥ä¸‹æ˜¯é›†æˆç¤ºä¾‹:

#### æ–¹æ³• 1: åœ¨ MultiLLMService ä¸­é›†æˆ

```csharp
public class MultiLLMService : IMultiLLMService
{
    private readonly ITokenUsageService _tokenUsageService;
    private readonly IHttpContextAccessor _httpContextAccessor;
    
    public async Task<string> GenerateReviewAsync(
        string code, 
        string context, 
        int? configurationId = null)
    {
        var stopwatch = System.Diagnostics.Stopwatch.StartNew();
        var configuration = await GetConfigurationAsync(configurationId);
        
        try
        {
            var provider = _providerFactory.CreateProvider(configuration);
            var fullPrompt = BuildReviewPrompt(code, context);
            
            // ä¼°ç®—è¾“å…¥ token
            var estimatedPromptTokens = _tokenUsageService.EstimateTokenCount(fullPrompt);
            
            // è°ƒç”¨ LLM
            var result = await provider.GenerateAsync(fullPrompt);
            stopwatch.Stop();
            
            // ä¼°ç®—è¾“å‡º token
            var estimatedCompletionTokens = _tokenUsageService.EstimateTokenCount(result);
            
            // è®°å½• token ä½¿ç”¨
            var userId = _httpContextAccessor.HttpContext?.User
                .FindFirstValue(ClaimTypes.NameIdentifier) ?? "system";
            
            await _tokenUsageService.RecordUsageAsync(
                userId: userId,
                projectId: null, // å¦‚æœæœ‰é¡¹ç›®ä¸Šä¸‹æ–‡,ä¼ å…¥projectId
                reviewRequestId: null, // å¦‚æœæœ‰reviewId,ä¼ å…¥reviewRequestId
                llmConfigurationId: configuration.Id,
                provider: configuration.Provider,
                model: configuration.Model,
                operationType: "Review",
                promptTokens: estimatedPromptTokens,
                completionTokens: estimatedCompletionTokens,
                isSuccessful: true,
                responseTimeMs: (int)stopwatch.ElapsedMilliseconds
            );
            
            return result;
        }
        catch (Exception ex)
        {
            stopwatch.Stop();
            
            // è®°å½•å¤±è´¥çš„è°ƒç”¨
            var userId = _httpContextAccessor.HttpContext?.User
                .FindFirstValue(ClaimTypes.NameIdentifier) ?? "system";
            
            await _tokenUsageService.RecordUsageAsync(
                userId: userId,
                projectId: null,
                reviewRequestId: null,
                llmConfigurationId: configuration.Id,
                provider: configuration.Provider,
                model: configuration.Model,
                operationType: "Review",
                promptTokens: 0,
                completionTokens: 0,
                isSuccessful: false,
                errorMessage: ex.Message,
                responseTimeMs: (int)stopwatch.ElapsedMilliseconds
            );
            
            throw;
        }
    }
}
```

#### æ–¹æ³• 2: ä½¿ç”¨è£…é¥°å™¨æ¨¡å¼

åˆ›å»ºä¸€ä¸ªè£…é¥°å™¨æ¥è‡ªåŠ¨è®°å½•æ‰€æœ‰ LLM è°ƒç”¨:

```csharp
public class TokenTrackingLLMService : IMultiLLMService
{
    private readonly IMultiLLMService _innerService;
    private readonly ITokenUsageService _tokenUsageService;
    
    public async Task<string> GenerateReviewAsync(...)
    {
        return await TrackUsageAsync(
            () => _innerService.GenerateReviewAsync(...),
            "Review",
            ...
        );
    }
    
    private async Task<string> TrackUsageAsync(
        Func<Task<string>> operation,
        string operationType,
        ...)
    {
        // è®°å½•é€»è¾‘
    }
}
```

### 2. æ³¨å†ŒæœåŠ¡

åœ¨ `Program.cs` ä¸­æ³¨å†ŒæœåŠ¡:

```csharp
// æ³¨å†Œ TokenUsageService
builder.Services.AddScoped<ITokenUsageService, TokenUsageService>();
builder.Services.AddScoped<ITokenUsageRepository, TokenUsageRepository>();

// æ³¨å†Œ HttpContextAccessor (ç”¨äºè·å–å½“å‰ç”¨æˆ·)
builder.Services.AddHttpContextAccessor();
```

### 3. æ•°æ®åº“è¿ç§»

åˆ›å»ºå¹¶åº”ç”¨æ•°æ®åº“è¿ç§»:

```bash
cd AIReview.API
dotnet ef migrations add AddTokenUsageTracking
dotnet ef database update
```

### 4. æ›´æ–° UnitOfWork

åœ¨ `UnitOfWork.cs` ä¸­æ·»åŠ  TokenUsageRepository:

```csharp
public class UnitOfWork : IUnitOfWork
{
    private ITokenUsageRepository? _tokenUsageRepository;
    
    public ITokenUsageRepository TokenUsageRepository =>
        _tokenUsageRepository ??= new TokenUsageRepository(_context, _loggerFactory.CreateLogger<TokenUsageRepository>());
}
```

## ğŸ“Š API ç«¯ç‚¹

### ç”¨æˆ·ç«¯ç‚¹

#### GET /api/v1/tokenusage/dashboard
è·å–ç”¨æˆ·çš„ token ä½¿ç”¨ä»ªè¡¨æ¿

**æŸ¥è¯¢å‚æ•°:**
- `startDate` (å¯é€‰): å¼€å§‹æ—¥æœŸ
- `endDate` (å¯é€‰): ç»“æŸæ—¥æœŸ
- `days` (å¯é€‰, é»˜è®¤30): æŸ¥è¯¢æœ€è¿‘Nå¤©

**å“åº”:**
```json
{
  "statistics": {
    "totalRequests": 150,
    "successfulRequests": 148,
    "failedRequests": 2,
    "totalTokens": 125000,
    "totalCost": 0.625,
    "successRate": 98.67,
    "cacheHitRate": 5.33
  },
  "providerStatistics": [
    {
      "provider": "OpenAI",
      "model": "gpt-4o-mini",
      "requestCount": 100,
      "totalTokens": 80000,
      "totalCost": 0.420
    }
  ],
  "operationStatistics": [
    {
      "operationType": "Review",
      "requestCount": 80,
      "totalTokens": 70000,
      "totalCost": 0.350
    }
  ],
  "dailyTrends": [
    {
      "date": "2025-10-17",
      "requestCount": 15,
      "totalTokens": 12000,
      "totalCost": 0.060
    }
  ],
  "recentRecords": [...]
}
```

#### GET /api/v1/tokenusage/records
è·å–ç”¨æˆ·çš„ token ä½¿ç”¨è®°å½•

**æŸ¥è¯¢å‚æ•°:**
- `page` (é»˜è®¤1): é¡µç 
- `pageSize` (é»˜è®¤50): æ¯é¡µæ•°é‡

#### GET /api/v1/tokenusage/projects/{projectId}/statistics
è·å–é¡¹ç›®çš„ token ä½¿ç”¨ç»Ÿè®¡

#### POST /api/v1/tokenusage/estimate
ä¼°ç®— token æ•°é‡å’Œæˆæœ¬

**è¯·æ±‚ä½“:**
```json
{
  "provider": "OpenAI",
  "model": "gpt-4o-mini",
  "text": "è¦åˆ†æçš„ä»£ç æˆ–æ–‡æœ¬",
  "estimatedCompletionTokens": 1000
}
```

**å“åº”:**
```json
{
  "estimatedPromptTokens": 2500,
  "estimatedCompletionTokens": 1000,
  "estimatedTotalTokens": 3500,
  "estimatedPromptCost": 0.000375,
  "estimatedCompletionCost": 0.0006,
  "estimatedTotalCost": 0.000975,
  "currency": "USD"
}
```

### ç®¡ç†å‘˜ç«¯ç‚¹

#### GET /api/v1/tokenusage/global/statistics
è·å–å…¨å±€ token ä½¿ç”¨ç»Ÿè®¡(ä»…ç®¡ç†å‘˜)

## ğŸ¨ å‰ç«¯é›†æˆ

### 1. åˆ›å»º TypeScript ç±»å‹

```typescript
// src/types/token-usage.ts
export interface TokenUsageStatistics {
  totalRequests: number;
  successfulRequests: number;
  failedRequests: number;
  totalTokens: number;
  totalCost: number;
  successRate: number;
  cacheHitRate: number;
}

export interface ProviderUsageStatistics {
  provider: string;
  model: string;
  requestCount: number;
  totalTokens: number;
  totalCost: number;
}

export interface DailyUsageTrend {
  date: string;
  requestCount: number;
  totalTokens: number;
  totalCost: number;
}

export interface TokenUsageDashboard {
  statistics: TokenUsageStatistics;
  providerStatistics: ProviderUsageStatistics[];
  operationStatistics: OperationUsageStatistics[];
  dailyTrends: DailyUsageTrend[];
  recentRecords: TokenUsageRecord[];
}
```

### 2. åˆ›å»º API æœåŠ¡

```typescript
// src/services/token-usage.service.ts
import { apiClient } from './api-client';
import { TokenUsageDashboard } from '../types/token-usage';

export class TokenUsageService {
  async getDashboard(days: number = 30): Promise<TokenUsageDashboard> {
    const response = await apiClient.get<TokenUsageDashboard>(
      `/tokenusage/dashboard?days=${days}`
    );
    return response;
  }
  
  async estimateCost(provider: string, model: string, text: string): Promise<CostEstimate> {
    const response = await apiClient.post<CostEstimate>('/tokenusage/estimate', {
      provider,
      model,
      text,
      estimatedCompletionTokens: 1000
    });
    return response;
  }
}

export const tokenUsageService = new TokenUsageService();
```

### 3. åˆ›å»ºä»ªè¡¨æ¿ç»„ä»¶

```typescript
// src/pages/TokenUsagePage.tsx
import React from 'react';
import { useQuery } from '@tanstack/react-query';
import { tokenUsageService } from '../services/token-usage.service';

export const TokenUsagePage: React.FC = () => {
  const { data: dashboard, isLoading } = useQuery({
    queryKey: ['token-usage-dashboard'],
    queryFn: () => tokenUsageService.getDashboard(30)
  });
  
  if (isLoading) return <div>åŠ è½½ä¸­...</div>;
  
  return (
    <div className="p-6">
      <h1 className="text-2xl font-bold mb-6">Token ä½¿ç”¨ç»Ÿè®¡</h1>
      
      {/* ç»Ÿè®¡å¡ç‰‡ */}
      <div className="grid grid-cols-4 gap-4 mb-6">
        <StatCard 
          title="æ€»è¯·æ±‚" 
          value={dashboard.statistics.totalRequests} 
        />
        <StatCard 
          title="æ€» Tokens" 
          value={dashboard.statistics.totalTokens.toLocaleString()} 
        />
        <StatCard 
          title="æ€»æˆæœ¬" 
          value={`$${dashboard.statistics.totalCost.toFixed(4)}`} 
        />
        <StatCard 
          title="æˆåŠŸç‡" 
          value={`${dashboard.statistics.successRate.toFixed(1)}%`} 
        />
      </div>
      
      {/* å›¾è¡¨ */}
      <div className="grid grid-cols-2 gap-6">
        <ProviderChart data={dashboard.providerStatistics} />
        <TrendChart data={dashboard.dailyTrends} />
      </div>
      
      {/* æœ€è¿‘è®°å½• */}
      <RecentRecordsTable records={dashboard.recentRecords} />
    </div>
  );
};
```

## ğŸ“ˆ ä½¿ç”¨åœºæ™¯

### 1. æˆæœ¬ç›‘æ§
- å®æ—¶è¿½è¸ª AI è°ƒç”¨æˆæœ¬
- è®¾ç½®æˆæœ¬é¢„è­¦é˜ˆå€¼
- ç”Ÿæˆæˆæœ¬æŠ¥å‘Š

### 2. æ€§èƒ½ä¼˜åŒ–
- è¯†åˆ«é«˜ token æ¶ˆè€—çš„æ“ä½œ
- ä¼˜åŒ– prompt è®¾è®¡
- å‡å°‘ä¸å¿…è¦çš„ AI è°ƒç”¨

### 3. ç”¨é‡åˆ†æ
- åˆ†æä¸åŒæ¨¡å‹çš„ä½¿ç”¨æƒ…å†µ
- æ¯”è¾ƒæä¾›å•†çš„æˆæœ¬æ•ˆç›Š
- è¯†åˆ«ä½¿ç”¨è¶‹åŠ¿

### 4. é¢„ç®—ç®¡ç†
- ä¸ºç”¨æˆ·/é¡¹ç›®è®¾ç½® token é…é¢
- é¢„æµ‹æœªæ¥æˆæœ¬
- ä¼˜åŒ–èµ„æºåˆ†é…

## ğŸ”„ æœªæ¥æ”¹è¿›

### çŸ­æœŸ
- [ ] æ·»åŠ  token é…é¢ç®¡ç†
- [ ] å®ç°æˆæœ¬é¢„è­¦é€šçŸ¥
- [ ] æ”¯æŒä» LLM API å“åº”ä¸­è·å–å‡†ç¡®çš„ token è®¡æ•°
- [ ] æ·»åŠ å¯¼å‡ºæŠ¥è¡¨åŠŸèƒ½

### ä¸­æœŸ
- [ ] å®ç° token ç¼“å­˜ç­–ç•¥
- [ ] æ·»åŠ æˆæœ¬ä¼˜åŒ–å»ºè®®
- [ ] æ”¯æŒæ›´å¤š LLM æä¾›å•†
- [ ] å®ç°æŒ‰å›¢é˜Ÿçš„æˆæœ¬åˆ†æ‘Š

### é•¿æœŸ
- [ ] AI é©±åŠ¨çš„æˆæœ¬ä¼˜åŒ–
- [ ] é¢„æµ‹æ€§æˆæœ¬åˆ†æ
- [ ] è‡ªåŠ¨åˆ‡æ¢æœ€ä¼˜æ¨¡å‹
- [ ] æˆæœ¬å¼‚å¸¸æ£€æµ‹

## ğŸ“ æ³¨æ„äº‹é¡¹

1. **Token ä¼°ç®—ç²¾åº¦**: å½“å‰ä½¿ç”¨ç®€å•çš„å­—ç¬¦æ•°/4çš„æ–¹å¼ä¼°ç®—,å®é™… token æ•°å¯èƒ½æœ‰åå·®ã€‚å»ºè®®åç»­é›†æˆ tiktoken æˆ–ä» API å“åº”ä¸­è·å–å‡†ç¡®å€¼ã€‚

2. **å®šä»·æ›´æ–°**: LLM æä¾›å•†çš„å®šä»·å¯èƒ½ä¼šå˜åŒ–,éœ€è¦å®šæœŸæ›´æ–° `LLMPricingService` ä¸­çš„å®šä»·ä¿¡æ¯ã€‚

3. **æ€§èƒ½è€ƒè™‘**: Token è®°å½•æ“ä½œåº”è¯¥æ˜¯å¼‚æ­¥çš„,ä¸åº”é˜»å¡ä¸»è¦çš„ LLM è°ƒç”¨æµç¨‹ã€‚

4. **éšç§ä¿æŠ¤**: Token ä½¿ç”¨è®°å½•å¯èƒ½åŒ…å«æ•æ„Ÿä¿¡æ¯,éœ€è¦é€‚å½“çš„è®¿é—®æ§åˆ¶å’Œæ•°æ®ä¿æŠ¤æªæ–½ã€‚

5. **æ•°æ®ä¿ç•™**: è€ƒè™‘å®æ–½æ•°æ®ä¿ç•™ç­–ç•¥,å®šæœŸå½’æ¡£æˆ–åˆ é™¤æ—§çš„ä½¿ç”¨è®°å½•ã€‚

## ğŸ§ª æµ‹è¯•

### å•å…ƒæµ‹è¯•ç¤ºä¾‹

```csharp
[Fact]
public async Task RecordUsageAsync_ShouldCalculateCostCorrectly()
{
    // Arrange
    var service = new TokenUsageService(_repository, _logger);
    
    // Act
    var record = await service.RecordUsageAsync(
        userId: "test-user",
        projectId: null,
        reviewRequestId: null,
        llmConfigurationId: 1,
        provider: "OpenAI",
        model: "gpt-4o-mini",
        operationType: "Review",
        promptTokens: 1000,
        completionTokens: 500
    );
    
    // Assert
    Assert.Equal(0.00015m, record.PromptCost); // $0.15 per 1M tokens
    Assert.Equal(0.0003m, record.CompletionCost); // $0.60 per 1M tokens
    Assert.Equal(0.00045m, record.TotalCost);
}
```

## ğŸ“ æ”¯æŒ

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®,è¯·è”ç³»å¼€å‘å›¢é˜Ÿæˆ–æäº¤ GitHub Issueã€‚
